#!/usr/bin/env python3
"""
Multi-turn vs Single-turn Prefix Cache Hit Rate Experiment
Parameter Sweep: Block Size, Block Number (Env), Eviction Policy (Env)
"""
import sys
import os
import json
import tempfile
import time
from pathlib import Path
from collections import defaultdict
from typing import Dict, Any

# =============================================================================
# 1. è·¯å¾„è®¾ç½®ä¸å¯¼å…¥
# =============================================================================
# å‡è®¾å½“å‰è„šæœ¬ä½äº milestones/milestone2/ ç›®å½• (æ ¹æ®æ‚¨ä¹‹å‰çš„è·¯å¾„æ¨æ–­)
sys.path.insert(0, str(Path(__file__).parent.parent))

from vllm import EngineArgs
from vllm.engine.llm_engine import LLMEngine
from transformers import AutoTokenizer

# å¯¼å…¥è‡ªå®šä¹‰ Tracker å’Œ Simulator
try:
    from correct_hit_rate_tracker import global_hit_rate_tracker
    from cache_block_tracker import global_cache_block_tracker
    from milestone2_code.client_simulator import ClientSimulator
except ImportError as e:
    print(f"âŒ Import Error: {e}")
    print("Please ensure 'correct_hit_rate_tracker.py' and 'milestone2_code/' are in the path.")
    sys.exit(1)

# æ¨¡å‹ä¸Traceè·¯å¾„
model_path = str(Path(__file__).parent.parent / "exported_models" / "Llama-3.2-1B-Instruct")
multi_turn_trace = str(Path(__file__).parent / "traces" / "sharegpt_multi_turn.jsonl")
single_turn_trace = str(Path(__file__).parent / "traces" / "sharegpt_single_turn.jsonl")

# =============================================================================
# 2. å®éªŒå‚æ•°é…ç½® (åœ¨æ­¤å¤„ä¿®æ”¹è¦æµ‹è¯•çš„èŒƒå›´)
# =============================================================================

# 1. Block Sizes (vLLM Engine Argument)
BLOCK_SIZES = [16, 128] 
# æ³¨æ„ï¼š1024ä½œä¸ºblock sizeå¯èƒ½è¿‡å¤§ï¼Œä¼šå¯¼è‡´ç¢ç‰‡åŒ–ï¼Œå»ºè®®æµ‹è¯• 16, 64, 128

# 2. Block Numbers (Passed via ENV: VLLM_TEST_BLOCK_NUMBER)
# è¿™äº›å€¼åº”è¯¥æ ¹æ® block_size è°ƒæ•´ï¼Œæˆ–è€…è®¾ä¸ºå›ºå®šå€¼ (ä»£è¡¨ GPU æ˜¾å­˜å¤§å°)
BLOCK_NUMBERS = [16, 64, 1024, 16384] 

# 3. Eviction Policies (Passed via ENV: VLLM_TEST_EVICTION_POLICY)
EVICTION_POLICIES = ["LRU", "LFU", "FIFO"]

# ç”Ÿæˆæµ‹è¯•ç»„åˆ
TEST_CONFIGS = []
for bs in BLOCK_SIZES:
    for bn in BLOCK_NUMBERS:
        for ep in EVICTION_POLICIES:
            TEST_CONFIGS.append({
                "block_size": bs,
                "block_number": bn,
                "eviction_policy": ep
            })

print("=" * 80)
print(f"Experimental Configuration Loaded")
print(f"Total Combinations: {len(TEST_CONFIGS)}")
print("=" * 80)

# =============================================================================
# 3. æ•°æ®å‡†å¤‡ (Data Preparation)
# =============================================================================
tokenizer = AutoTokenizer.from_pretrained(model_path)
MAX_TOKENS = 2048

print("\nã€Step 1ã€‘Preparing Trace Files...")

# A. è¿‡æ»¤ Multi-turn
conversations = defaultdict(list)
with open(multi_turn_trace, 'r') as f:
    for line in f:
        entry = json.loads(line.strip())
        conv_id = entry.get('conversation_id', 'unknown')
        prompt_tokens = tokenizer.encode(entry['prompt'], add_special_tokens=False)
        entry['token_count'] = len(prompt_tokens)
        conversations[conv_id].append(entry)

filtered_multi_convs = {}
for conv_id, turns in conversations.items():
    if len(turns) >= 2 and all(turn['token_count'] <= MAX_TOKENS for turn in turns):
        filtered_multi_convs[conv_id] = turns

# é€‰æ‹© Conversation æ•°é‡ (å…¨é‡æˆ–éƒ¨åˆ†)
NUM_CONVS_TO_TEST = len(filtered_multi_convs) # run all
# NUM_CONVS_TO_TEST = 20 # fast debug
selected_convs = dict(list(filtered_multi_convs.items())[:NUM_CONVS_TO_TEST])
total_requests = sum(len(turns) for turns in selected_convs.values())

fd_multi, filtered_multi_trace_path = tempfile.mkstemp(suffix='_multi.jsonl')
with open(filtered_multi_trace_path, 'w') as f:
    for conv_id in sorted(selected_convs.keys()):
        for turn in selected_convs[conv_id]:
            f.write(json.dumps(turn) + '\n')
os.close(fd_multi)

# B. è¿‡æ»¤ Single-turn (åŒ¹é… Request æ•°é‡)
single_requests = []
with open(single_turn_trace, 'r') as f:
    for line in f:
        entry = json.loads(line.strip())
        if len(tokenizer.encode(entry['prompt'], add_special_tokens=False)) <= MAX_TOKENS:
            single_requests.append(entry)
        if len(single_requests) >= total_requests:
            break

fd_single, filtered_single_trace_path = tempfile.mkstemp(suffix='_single.jsonl')
with open(filtered_single_trace_path, 'w') as f:
    for entry in single_requests:
        f.write(json.dumps(entry) + '\n')
os.close(fd_single)

print(f"âœ“ Data ready.")
print(f"  Multi-turn path: {filtered_multi_trace_path}")
print(f"  Single-turn path: {filtered_single_trace_path}")
print(f"  Total Requests per run: {total_requests}")


# =============================================================================
# 4. å®éªŒæ ¸å¿ƒé€»è¾‘
# =============================================================================
def run_single_experiment(
    trace_path: str,
    is_multi_turn: bool,
    config: Dict[str, Any]
) -> Dict[str, Any]:
    
    bs = config['block_size']
    bn = config['block_number']
    ep = config['eviction_policy']
    mode_name = "Multi-Turn" if is_multi_turn else "Single-Turn"

    print(f"\n--- Running: {mode_name} | BS={bs} | BN={bn} | Policy={ep} ---")

    # 1. è®¾ç½®ç¯å¢ƒå˜é‡ (Injecting into vLLM)
    os.environ["VLLM_TEST_BLOCK_NUMBER"] = str(bn)
    os.environ["VLLM_TEST_EVICTION_POLICY"] = ep
    
    # è®¾ç½® Trace è·¯å¾„ä¾› Tracker ä½¿ç”¨ (å¦‚æœéœ€è¦)
    os.environ["VLLM_SIM_TRACE_PATH"] = trace_path

    # 2. é‡ç½® Trackers
    global_hit_rate_tracker.reset()
    global_cache_block_tracker.reset()

    # 3. åˆå§‹åŒ– Engine
    # æ³¨æ„: æˆ‘ä»¬ä½¿ç”¨ CPU æ¨¡å¼ä»¥é¿å…æ˜¾å­˜ OOMï¼Œå¹¶é€šè¿‡ Env Var å¼ºåˆ¶è®¾ç½®å†…éƒ¨çš„ Block Number
    engine_args = EngineArgs(
        model=model_path,
        tokenizer=model_path,
        device="cpu", 
        max_model_len=2048,
        max_num_seqs=1,
        block_size=bs, # é€šè¿‡å‚æ•°ä¼ é€’ Block Size
        enable_prefix_caching=True,
        gpu_memory_utilization=0.9, # CPUæ¨¡å¼ä¸‹è¿™ä¸ªå‚æ•°å½±å“è¾ƒå°ï¼Œä½†ä¹Ÿä¿ç•™
        enforce_eager=True # ç®€åŒ–å›¾æ‰§è¡Œ
    )
    
    try:
        engine = LLMEngine.from_engine_args(engine_args)
    except Exception as e:
        print(f"âŒ Engine Init Failed: {e}")
        return None

    # 4. åˆå§‹åŒ– Simulator
    simulator = ClientSimulator(
        trace_path=trace_path,
        tokenizer=tokenizer,
        arrival_rate=1.0,
    )

    # 5. è¿è¡Œæ¨¡æ‹Ÿ
    start_t = time.time()
    if is_multi_turn:
        # Conversation æ¨¡å¼: é€ä¸ªå¯¹è¯å¤„ç†ï¼Œå…è®¸å¤ç”¨
        simulator.send_requests_conversation_by_conversation(engine, max_steps_per_turn=5000)
    else:
        # Standard æ¨¡å¼: ä¸€æ¬¡æ€§å‘é€æ‰€æœ‰
        simulator.send_requests_to_engine(engine)
        simulator.run_engine_until_complete(engine, max_steps=10000)
    
    duration = time.time() - start_t

    # 6. æ”¶é›†ç»“æœ
    stats = global_hit_rate_tracker.get_stats()
    
    # è·å– vLLM å†…éƒ¨ç»Ÿè®¡ (å¦‚æœæ˜¯ GPU æ¨¡å¼æ‰æœ‰æ•ˆï¼ŒCPU æ¨¡å¼ä¸‹é€šå¸¸è¿”å› -1 æˆ– 0)
    # æˆ‘ä»¬ä¸»è¦ä¾èµ– global_hit_rate_tracker çš„é€»è¾‘ç»Ÿè®¡
    from vllm.utils import Device
    vllm_gpu_hit_rate = engine.scheduler[0].get_prefix_cache_hit_rate(Device.GPU)
    
    # 7. æ¸…ç†ç¯å¢ƒå˜é‡
    os.environ.pop("VLLM_TEST_BLOCK_NUMBER", None)
    os.environ.pop("VLLM_TEST_EVICTION_POLICY", None)
    
    # æ˜¾å¼é‡Šæ”¾ engine èµ„æº (å°½é‡)
    del engine
    import gc
    gc.collect()

    return {
        "hit_rate": stats['overall_hit_rate'],
        "total_requests": stats['total_requests'],
        "vllm_internal_hit_rate": vllm_gpu_hit_rate,
        "duration": duration,
        "detailed_stats": stats
    }

# =============================================================================
# 5. ä¸»å¾ªç¯ (Main Loop)
# =============================================================================
results_summary = []

print("\nğŸš€ Starting Parameter Sweep...")

for i, config in enumerate(TEST_CONFIGS):
    print(f"\n[{i+1}/{len(TEST_CONFIGS)}] Processing Configuration...")
    
    # Run Single Turn
    single_res = run_single_experiment(filtered_single_trace_path, False, config)
    
    # Run Multi Turn
    multi_res = run_single_experiment(filtered_multi_trace_path, True, config)
    
    if single_res and multi_res:
        # Calculate Improvement
        s_rate = single_res['hit_rate']
        m_rate = multi_res['hit_rate']
        imp = m_rate - s_rate
        
        entry = {
            "config": config,
            "single_turn": {
                "hit_rate": s_rate,
                "duration": single_res['duration']
            },
            "multi_turn": {
                "hit_rate": m_rate,
                "duration": multi_res['duration']
            },
            "improvement": imp
        }
        results_summary.append(entry)
        
        print(f"   >>> Result: Single={s_rate:.2%} | Multi={m_rate:.2%} | Diff={imp:+.2%}")
    else:
        print("   >>> Result: FAILED")

# =============================================================================
# 6. ç»“æœè¾“å‡ºä¸ä¿å­˜
# =============================================================================
print("\n" + "="*100)
print(f"{'BS':<5} | {'BN':<8} | {'Policy':<6} | {'Single Turn':<12} | {'Multi Turn':<12} | {'Improvement':<12}")
print("-" * 100)

for res in results_summary:
    c = res['config']
    print(f"{c['block_size']:<5} | {c['block_number']:<8} | {c['eviction_policy']:<6} | "
          f"{res['single_turn']['hit_rate']:<12.2%} | {res['multi_turn']['hit_rate']:<12.2%} | "
          f"{res['improvement']:<+12.2%}")

print("-" * 100)

output_file = Path(__file__).parent / "task2_results_sweep.json"
with open(output_file, 'w') as f:
    json.dump(results_summary, f, indent=2)

print(f"\nâœ“ Results saved to: {output_file}")

# Cleanup
if os.path.exists(filtered_multi_trace_path):
    os.unlink(filtered_multi_trace_path)
if os.path.exists(filtered_single_trace_path):
    os.unlink(filtered_single_trace_path)

print("Done.")