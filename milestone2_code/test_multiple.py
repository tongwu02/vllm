#!/usr/bin/env python3
"""
Task 1: Baseline Performance Testing (Fixed Adapter Version)
ä¿®å¤äº† KeyError: 'prompt' é—®é¢˜ã€‚
è‡ªåŠ¨å°† 'messages' æ ¼å¼è½¬æ¢ä¸º Simulator éœ€è¦çš„ 'prompt' æ ¼å¼ã€‚
"""
import sys
import os
import json
import time
import gc
from pathlib import Path
from typing import Dict, Any, List

# =============================================================================
# 1. Path Setup
# =============================================================================
sys.path.insert(0, str(Path(__file__).parent.parent))

from vllm import EngineArgs
from vllm.engine.llm_engine import LLMEngine
from transformers import AutoTokenizer

try:
    from correct_hit_rate_tracker import global_hit_rate_tracker
    from cache_block_tracker import global_cache_block_tracker
    from milestone2_code.client_simulator import ClientSimulator
except ImportError as e:
    print(f"âŒ Import Error: {e}")
    sys.exit(1)

# Model Path
model_path = str(Path(__file__).parent.parent / "exported_models" / "Llama-3.2-1B-Instruct")
TRACE_DIR = Path(__file__).parent / "traces"

# æŒ‡å‘ä½ åˆšæ‰ç”Ÿæˆçš„ Clean æ–‡ä»¶
DATASETS = {
    "ShareGPT": TRACE_DIR / "sharegpt_multi_turn.jsonl",
    "AgentBank": TRACE_DIR / "agentbank_multi_turn.jsonl",
    "CC":        TRACE_DIR / "ccbench_multi_turn.jsonl" 
}

# =============================================================================
# 2. Configuration
# =============================================================================

# å–æ ·æ•°é‡ï¼š500æ¡å¯¹è¯è¶³ä»¥æµ‹å‡ºç¼“å­˜æ€§èƒ½
MAX_REQUESTS_PER_DATASET = 500 
MAX_TOKENS = 8192

BLOCK_SIZES = [16]
BLOCK_NUMBERS = [32, 28] 
EVICTION_POLICIES = ["LRU", "LFU", "FIFO", "PROTECTED_LRU"]

TEST_CONFIGS = []
for bs in BLOCK_SIZES:
    for bn in BLOCK_NUMBERS:
        for ep in EVICTION_POLICIES:
            TEST_CONFIGS.append({
                "block_size": bs,
                "block_number": bn,
                "eviction_policy": ep
            })

print("=" * 80)
print(f"Task 1: Baseline Sweep (Adapter Mode)")
print(f"Max Requests per Dataset: {MAX_REQUESTS_PER_DATASET}")
print("=" * 80)

# åŠ è½½ Tokenizer (ç”¨äºæŠŠ messages è½¬æˆ prompt string)
tokenizer = AutoTokenizer.from_pretrained(model_path)

# =============================================================================
# 3. Helper: Smart Trace Adapter (å…³é”®ä¿®å¤)
# =============================================================================
def get_sample_trace(name: str, full_path: Path, limit: int) -> str:
    """
    è¯»å– Clean æ ¼å¼ (messages list) çš„æ–‡ä»¶ï¼Œ
    å°†å…¶è½¬æ¢ä¸º Simulator éœ€è¦çš„ Trace æ ¼å¼ (prompt string)ï¼Œ
    å¹¶åªä¿å­˜å‰ limit æ¡åˆ°ä¸´æ—¶æ–‡ä»¶ã€‚
    """
    temp_path = TRACE_DIR / f"temp_{name}_{limit}.jsonl"
    print(f"   -> Converting & Sampling {name} to {temp_path}...")
    
    processed_count = 0
    
    with open(full_path, 'r', encoding='utf-8') as f_in, \
         open(temp_path, 'w', encoding='utf-8') as f_out:
        
        for line in f_in:
            if processed_count >= limit:
                break
                
            try:
                data = json.loads(line)
                # å…¼å®¹æ€§æ£€æŸ¥ï¼šå¦‚æœå·²ç»æ˜¯ prompt æ ¼å¼å°±ç›´æ¥ç”¨ï¼Œå¦‚æœæ˜¯ messages å°±è½¬æ¢
                if "messages" in data:
                    messages = data["messages"]
                    conversation_id = data.get("conversation_id", f"conv_{processed_count}")
                    
                    # --- æ ¸å¿ƒè½¬æ¢é€»è¾‘ ---
                    # æˆ‘ä»¬éœ€è¦æŠŠæ•´ä¸ªå¯¹è¯æ‹†è§£æˆ Simulator èƒ½çœ‹æ‡‚çš„ Single Turn åºåˆ—
                    # Simulator ä¼šæ ¹æ® conversation_id è‡ªåŠ¨æŠŠå®ƒä»¬ä¸²èµ·æ¥
                    
                    history = []
                    turn_index = 0
                    
                    for msg in messages:
                        if msg['role'] == 'system':
                            history.append(msg)
                            continue
                            
                        if msg['role'] == 'user':
                            # æ„é€ å½“å‰è½®æ¬¡çš„ Promptï¼šå†å² + å½“å‰é—®é¢˜
                            current_input = history + [msg]
                            
                            # ä½¿ç”¨ chat_template å˜æˆå­—ç¬¦ä¸² 
                            prompt_str = tokenizer.apply_chat_template(
                                current_input, 
                                tokenize=False, 
                                add_generation_prompt=True
                            )
                            
                            # === [å…³é”®ä¿®æ”¹] é•¿åº¦æ£€æŸ¥ ===
                            # ä½¿ç”¨å…¨å±€å¸¸é‡ MAX_TOKENS
                            token_ids = tokenizer.encode(prompt_str, add_special_tokens=False)
                            if len(token_ids) > MAX_TOKENS:
                                # å¦‚æœè¿™ä¸€è½®å¤ªé•¿ï¼Œè·³è¿‡è¿™ä¸€è½® (æˆ–è€…ä½ å¯ä»¥é€‰æ‹© break è·³è¿‡æ•´ä¸ªå¯¹è¯)
                                print(f"      [Warn] Skipping a turn > {MAX_TOKENS} tokens.")
                                continue 
                            # ============================
                            
                            # å†™å…¥ Trace æ¡ç›®
                            entry = {
                                "conversation_id": conversation_id,
                                "turn_index": turn_index,
                                "prompt": prompt_str,
                                "response": "" # Simulator è¿è¡Œæ—¶ä¸éœ€è¦çœŸå® response
                            }
                            f_out.write(json.dumps(entry, ensure_ascii=False) + "\n")
                            
                            # æ›´æ–°å†å²å’Œè½®æ•°
                            history.append(msg)
                            turn_index += 1
                            
                        elif msg['role'] == 'assistant':
                            history.append(msg)
                    
                    processed_count += 1
                    
                else:
                    # å¦‚æœå·²ç»æ˜¯æ—§æ ¼å¼ï¼Œç›´æ¥å†™
                    f_out.write(line)
                    processed_count += 1
                    
            except Exception as e:
                print(f"Warning: failed to convert line: {e}")
                continue
            
    return str(temp_path)

# =============================================================================
# 4. Core Experiment
# =============================================================================
def run_single_experiment(
    dataset_name: str,
    trace_file: str,
    config: Dict[str, Any]
) -> Dict[str, Any]:
    
    bs = config['block_size']
    bn = config['block_number']
    ep = config['eviction_policy']
    capacity_tokens = bs * bn

    print(f"\n--- [{dataset_name}] BS={bs} | BN={bn} | Policy={ep} ---")

    # Env Vars
    os.environ["VLLM_TEST_BLOCK_NUMBER"] = str(bn)
    os.environ["VLLM_TEST_EVICTION_POLICY"] = ep
    os.environ["VLLM_SIM_TRACE_PATH"] = str(trace_file)

    # Reset
    global_hit_rate_tracker.reset()
    global_cache_block_tracker.reset()

    # Engine
    engine_args = EngineArgs(
        model=model_path,
        tokenizer=model_path,
        device="cpu", 
        max_model_len=MAX_TOKENS,
        max_num_seqs=1,
        block_size=bs,
        enable_prefix_caching=True,
        gpu_memory_utilization=0.9, 
        enforce_eager=True
    )
    
    try:
        engine = LLMEngine.from_engine_args(engine_args)
    except Exception as e:
        print(f"âŒ Engine Init Failed: {e}")
        return None

    # Simulator
    simulator = ClientSimulator(
        trace_path=str(trace_file),
        tokenizer=tokenizer,
        arrival_rate=1.0,
    )

    start_t = time.time()
    # è¿™é‡Œçš„ simulator ç°åœ¨èƒ½è¯»åˆ°æ­£ç¡®çš„ prompt å­—æ®µäº†
    simulator.send_requests_conversation_by_conversation(engine, max_steps_per_turn=5000)
    duration = time.time() - start_t

    stats = global_hit_rate_tracker.get_stats()
    
    os.environ.pop("VLLM_TEST_BLOCK_NUMBER", None)
    os.environ.pop("VLLM_TEST_EVICTION_POLICY", None)
    del engine
    gc.collect() 

    return {
        "dataset": dataset_name,
        "hit_rate": stats['overall_hit_rate'],
        "duration": duration,
        "config": config,
    }

# =============================================================================
# 5. Main Loop
# =============================================================================
results_summary = []

for dataset_name, path in DATASETS.items():
    if not os.path.exists(path):
        print(f"âš ï¸ Skipping {dataset_name}: {path} not found.")
        continue
        
    print(f"\n" + "="*60)
    print(f"ğŸ“‚ Processing: {dataset_name}")
    
    # è¿™ä¸€æ­¥ç°åœ¨ä¼šè‡ªåŠ¨å¤„ç†æ ¼å¼è½¬æ¢ï¼Œç”Ÿæˆå¸¦ 'prompt' å­—æ®µçš„ä¸´æ—¶æ–‡ä»¶
    temp_trace = get_sample_trace(dataset_name, path, MAX_REQUESTS_PER_DATASET)
    
    print("="*60)

    for config in TEST_CONFIGS:
        res = run_single_experiment(dataset_name, temp_trace, config)
        if res:
            results_summary.append(res)
            print(f"   >>> Result: Hit Rate={res['hit_rate']:.2%} | Duration={res['duration']:.2f}s")
    
    # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
    if os.path.exists(temp_trace):
        os.remove(temp_trace)

# =============================================================================
# 6. Save
# =============================================================================
print("\n" + "="*100)
print(f"{'Dataset':<12} | {'BN':<5} | {'Policy':<14} | {'Hit Rate':<10}")
print("-" * 100)

for res in results_summary:
    d = res['dataset']
    c = res['config']
    print(f"{d:<12} | {c['block_number']:<5} | {c['eviction_policy']:<14} | {res['hit_rate']:<10.2%}")

output_file = Path(__file__).parent / "task1_results_sweep.json"
with open(output_file, 'w') as f:
    json.dump(results_summary, f, indent=2)

print(f"\nâœ“ Results saved to: {output_file}")